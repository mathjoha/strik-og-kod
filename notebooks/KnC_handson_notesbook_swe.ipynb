{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/mathjoha/strik-og-kod/blob/main/notebooks/KnC_handson_notesbook_swe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
                "[![Open In Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mathjoha/strik-og-kod.git/main?labpath=notebooks%2FKnC_handson_notesbook_swe.ipynb) "
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ByJpAFyOvr70"
            },
            "source": [
                "# Knit & Code\n",
                "author: \"Mathias Johansson & Max Odsbjerg Pedersen\"\n",
                "\n",
                "date: \"2025-04-06\"\n",
                "\n",
                "Detta dokument består av den kod-delen av workshoppen \"Knit and Code\" vid\n",
                "Humanistiska och Teologiska Fakulteterna vid Lunds universitet utvecklad I\n",
                "samarbete med _AU Bibliotek vid Det Kongelige Bibliotek_. Workshopen handlar om\n",
                "att dra paralleller mellan stickning och kodning. \"Kodning\" förstås här som\n",
                "kopplingen mellan kodningsbaserad datahantering och ligger därför inom området\n",
                "datorvetenskap. Eftersom workshopen är gjord i sammanhang av humaniora avser\n",
                "exemplet _textmining_. När man använder _textmining_ är det primära intresset\n",
                "att extrahera information ur stora korpus - vilket är exakt det intresse som\n",
                "många humanister har.\n",
                "\n",
                "Inget recept är komplett utan en bild av den färdiga produkten som en av de\n",
                "första punkterna. Och detta är inget undantag. Slutresultatet i slutet av detta\n",
                "dokument är visualiseringen som visas precis under denna paragraf. Den visar de\n",
                "vanligast förekommande orden i gamla tidningsartiklar om stickning efter att\n",
                "alla stoppord har tagits bort (det, att, till, och, i - ord som inte bär någon\n",
                "större betydelse).\n",
                "\n",
                "![](https://github.com/mathjoha/strik-og-kod/blob/53a64caa55cebfea9e3d9e2db1f7305d1043d129/notebooks/swe_wc.png?raw=true)\n",
                "\n",
                "Yttrade ABM-ord och deras sällskap.\n",
                "\n",
                "<br>\n",
                "\n",
                "\n",
                "\n",
                "## Ladda ner och installera Python paket\n",
                "Vi arbetar i programmeringsspråket [Python ](https://www.python.org/), ett\n",
                "gratis och _open-source_ programmeringsspråk. Python får mest av sin\n",
                "funktionalitet genom att importera 'bibliotek', och python har ett mycket brett\n",
                "ekosystem med bibliotek som erbjuder nästan all funktionalitet du kan tänka\n",
                "dig. Bland annat många möjligheter för att bearbeta text, statistik och grafisk\n",
                "presentation av resultaten. Python får mest av sin funktionalitet genom att\n",
                "importera 'bibliotek', och python har ett mycket starkt ekosystem med bibliotek\n",
                "som erbjuder nästan alla funktioner du kan tänka dig. Bland annat många\n",
                "möjligheter för att bearbeta text, statistik och grafisk presentation av\n",
                "resultaten.\n",
                "\n",
                "I denna workshop är de relevanta paketen:\n",
                "- Pandas: Ett kraftfullt bibliotek för datahantering.\n",
                "- Wordcloud: Ett Python-bibliotek för att generera ordmoln från text.\n",
                "- Matplotlib: Ett bibliotek för att skapa statiska, animerade och interaktiva\n",
                "  visualiseringar i Python.\n",
                "\n",
                "Vi kommer att installera dessa paket med hjälp av pip, Pythons pakethanterare.\n",
                "Pip är ett kommandoradsverktyg som gör det lätt att installera och hantera\n",
                "Python-paket.\n",
                "\n",
                "Vi använder också två bibliotek från _standardbiblioteket_:\n",
                "- re: Ett reguljärt uttrycksmodul -- för textmönstermatchning.\n",
                "- Counter: Ett objekt för att räkna förekomster av objekt.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "3vBjTYxQvr75",
                "outputId": "4fa23bab-5852-4908-d17e-3c7d7d17bc4f"
            },
            "outputs": [],
            "source": [
                "print(\"install and load libraries\")\n",
                "!python -m pip install pandas wordcloud matplotlib\n",
                "\n",
                "import re\n",
                "import pandas as pd\n",
                "from collections import Counter\n",
                "from wordcloud import WordCloud\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "2BGwm7VGvr77"
            },
            "source": [
                "## Data – uttalanden about ABM\n",
                "\n",
                "The första vi behöver är någon text att behandla. Vi kommer här att använda\n",
                "_uttalanden_ från Svenska riksdagsdebatter från 1867-2022. För att hitta och\n",
                "komma åt dessa _uttalanden_ kommer vi först använda\n",
                "[Riksdagsdebatter.se](https://riksdagsdebatter.se/public/index.html#/about) som\n",
                "erbjuder ett grafiskt gränssnitt för sökning och åtkomst till debatterna. För\n",
                "att hitta alla uttalanden som nämner ABM söker vi efter följande tre nyckelord:\n",
                "\n",
                "\n",
                "- `arkiv*`\n",
                "- `bibliotek*`\n",
                "- `muse*`\n",
                "\n",
                "Asterisk `*` markerar trunkering -- expanderar för att hitta *alla ord* som börjar med\n",
                "`arkiv`, `bibliotek` eller `muse` (1477 ord enligt GUI).\n",
                "\n",
                "`Riksdagsdebatter.se` erbjuder inte något API och de ~9k resultaten är\n",
                "paginerade vid 50 resultat per sida. Men källmaterialet finns tillgängligt på:\n",
                "[the-swedish-parliament-corpus](https://github.com/swerik-project/the-swedish-parliament-corpus)\n",
                "where one can download the entire corpus and all speaker-metadata. In där man\n",
                "kan ladda ner hela korpusen och all talarmetadata. I förberedelse för denna\n",
                "workshop har jag laddat ner posterna, filterat bort alla uttalanden som INTE\n",
                "nämner minst ett av våra nyckelord och omstrukturerat resultatet till en\n",
                "CSV-fil.\n",
                "[skript](https://gist.github.com/mathjoha/edcdaf57c5c2d58d9f6b58a6350b811d)\n",
                "[korpus](https://raw.githubusercontent.com/mathjoha/strik-og-kod/refs/heads/main/the-swedish-parliament-corpus_ABM_.csv)\n",
                "\n",
                "\n",
                "\n",
                "### Hämta data\n",
                "\n",
                "För att komma åt datat för workshoppen behöver det laddas ner, vilket \n",
                "lätt kan göras med `wget`-programmet så här:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "J1HWgvDCvr79",
                "outputId": "8788e654-59ed-4e2d-e6ad-cc388cad4121"
            },
            "outputs": [],
            "source": [
                "!wget -c -nc \"https://github.com/mathjoha/strik-og-kod/blob/08f31ff60c9460efb232fecc1fbd07aace62dab4/data/the-swedish-parliament/the-swedish-parliament-corpus_ABM_.csv\" -O corpus.csv"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "yPpYXyFMvr8A"
            },
            "source": [
                "Then we use the function `read_csv` from the pandas library to load the file's contents into a DataFrame and keep that in memory under the variable name \"strik\".\n",
                "A DataFrame is comparable to spreadsheet in that it is a large matrix that stores data we work with.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "MYSueAQ2vr8B"
            },
            "outputs": [],
            "source": [
                "riksdag = pd.read_csv(\"corpus.csv\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "jMZ2G_Esvr8D"
            },
            "source": [
                "This gives us a new Pandas DataFrame named ”riksdag\" and containing 13755 rows and 5 columns.\n",
                "\n",
                "What is especially interesting for us is the column “content” – This is\n",
                "where the transcribed utterenaces are stored. Some of this text will not be easy\n",
                "on the eyes as they are filled with errors, and it is here where you meet the\n",
                "first downside of working with digitised text: OCR-errors.\n",
                "\n",
                "To understand why these errors occur it is necessary to turn towards the\n",
                "digitalization. In this process the protocols are scanned and processed \n",
                "with an OCR engine. These engines tend to work in two steps:\n",
                "\n",
                "  1. Segmenting the image into different blocks of text.\n",
                "  2. Transcribing the blocks of text into. \n",
                "\n",
                "These engines are typicall developed and tested on newer material -- part becase the older material is more complex, and there is less of it available. \n",
                "The team behin `The Swedish Parliament Corpus` have dedicated a lot of effort into developing systems for transcribing the older debates, identifying speakers and mappint them to metadata. Still, many [OCR-errors remain](https://github.com/swerik-project/pyriksdagen/blob/5cdc0875b7ed9a46f6ec7039d439d6e22e6acf54/examples/corpus-walkthrough.ipynb) in the material\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "## The Text mining task\n",
                "\n",
                "\n",
                "First we will convert the text into lowercase and split it into words using [Regular expression](https://en.wikipedia.org/wiki/Regex).\n",
                "We store these `lists` of lowercase words in the DataFrame in the column `word` and we expand this column into a new dataframe where each word has its own row.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "ofFT3Lzovr8F"
            },
            "outputs": [],
            "source": [
                "riksdag['word'] = riksdag.content.apply(lambda x: re.findall(r'\\w+', x.lower()))\n",
                "riksdag_tidy = riksdag.explode('word')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "P1_QcrU1vr8G"
            },
            "source": [
                "Let us just print out the new data frame to see how the tidytext format looks in practice. This is achieved by writing the name of the data frame:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "iORiX19zvr8I",
                "outputId": "fb300f1b-83c3-4482-dd9f-50db5c3b3b79"
            },
            "outputs": [],
            "source": [
                "riksdag_tidy"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "ZbMvJlVPvr8J"
            },
            "source": [
                "If we flip through the columns (with the little black arrow in the top-right corner) the last column will now be “word” which only contains single words.\n",
                "\n",
                "## Analysis\n",
                "\n",
                "### Wordcloud\n",
                "\n",
                "To get an overview of our dataset we will begin by counting the most used words in the article about knitting in the period 1845 to 1850:\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "qy62aN8avr8K",
                "outputId": "02ae2e3c-c1ef-46a1-9f52-356d1cd1c0ef",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "Counter(riksdag_tidy.word.values).most_common()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "hIASMASdvr8L"
            },
            "source": [
                "<br> To no one’s surprise most frequent words in the dataset is the grammatical particles. One way to negate these words is by using a stopword list which can be used to remove unwanted words. For this we will use a stopwords-list published publicly by [@peterdalle](https://gist.github.com/peterdalle):"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "XxQgt-lTvr8M",
                "outputId": "1a795c39-cbda-494f-e5c6-5ea7b61bf980"
            },
            "outputs": [],
            "source": [
                "!wget \"https://gist.githubusercontent.com/peterdalle/8865eb918a824a475b7ac5561f2f88e9/raw/cc1d05616e489576c1b934289711f041ff9b2281/swedish-stopwords.txt\" -O stopord.txt"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "KSiSK5ePvr8N"
            },
            "outputs": [],
            "source": [
                "with open('stopord.txt', 'r', encoding='utf-8') as f:\n",
                "  stopord = f.read().split('\\n')\n",
                "stopord += ['icke', 'af', 'herr', 'talman', 'år', 'ju', 't', '000']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "0aGkWL8mvr8N"
            },
            "source": [
                "<br>\n",
                "\n",
                "\n",
                "We will filter out all the stopwords !!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "i00WiV0Rvr8O",
                "outputId": "25cb0732-3149-4bab-e590-587b211b8f35",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "def not_stopword(word):\n",
                "  return word not in stopord\n",
                "\n",
                "words = Counter(filter(not_stopword, riksdag_tidy.word.values)).most_common()\n",
                "words"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "CpmR4_6avr8P"
            },
            "source": [
                "<br> We can already see quite a few interesting words. Something points\n",
                "towards a connection between maids that are seeking “condition” which back\n",
                "in the day meant a “service position” or a space of sorts. We can also see\n",
                "an OCR-error “eondition” and another spelling of condition, “kondition”.\n",
                "\n",
                "But a list is a little boring to look at. Could we perhaps create a\n",
                "beautiful wordcloud? Of course we can!\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "-n-LEhX9vr8P",
                "outputId": "57348a28-66b6-4de2-fd59-f97cc052d74b",
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "wc = WordCloud()\n",
                "wc.generate_from_frequencies(\n",
                "  Counter(filter(not_stopword, riksdag_tidy.word.values))).to_file('swe_wc.png')\n",
                "plt.imshow(wc, interpolation=\"bilinear\")\n",
                "plt.axis(\"off\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "kTLUwFB3vr8R"
            },
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "colab": {
            "include_colab_link": true,
            "provenance": []
        },
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.13.2"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
