{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/mathjoha/strik-og-kod/blob/main/notebooks/KnC_handson_notesbook_swe.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "[![Open In Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/mathjoha/strik-og-kod.git/main?labpath=notebooks%2FKnC_handson_notesbook_swe.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ByJpAFyOvr70"
   },
   "source": [
    "# Knit & Code\n",
    "author: \"Mathias Johansson & Max Odsbjerg Pedersen\"\n",
    "\n",
    "date: \"2025-04-06\"\n",
    "\n",
    "Detta dokument består av den kod-delen av workshoppen \"Knit and Code\" vid Humanistiska och Teologiska Fakulteterna vid Lunds universitet utvecklad I samarbete med _AU Bibliotek vid Det Kongelige Bibliotek_. Workshopen handlar om att dra paralleller mellan stickning och kodning. \"Kodning\" förstås här som kopplingen mellan kodningsbaserad datahantering och ligger därför inom området datorvetenskap. Eftersom workshopen är gjord i sammanhang av humaniora avser exemplet _textmining_. När man använder _textmining_ är det primära intresset att extrahera information ur stora korpus - vilket är exakt det intresse som många humanister har.\n",
    "\n",
    "# <Todo>\n",
    "No recipe is complete without a picture of the final product as one of the first items. And this is no exception. The final result at the end of this document is the visualisation shown just under this paragraph. It shows the most frequently appearing words in old newspaper articles concerning knitting after all stopwords has been removed (it, that, to, and, in - words which bear no larger meaning).\n",
    "\n",
    "![](https://github.com/mathjoha/strik-og-kod/blob/53a64caa55cebfea9e3d9e2db1f7305d1043d129/notebooks/swe_wc.png)\n",
    "\n",
    "Knitting words and words which accompany them.\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "# </Todo>\n",
    "\n",
    "\n",
    "\n",
    "## Ladda ner och installera Python paket\n",
    "Vi arbetar i programmeringsspråket [Python ](https://www.python.org/), ett\n",
    "gratis och _open-source_ programmeringsspråk. Python får mest av sin\n",
    "funktionalitet genom att importera 'bibliotek', och python har ett mycket brett\n",
    "ekosystem med bibliotek som erbjuder nästan all funktionalitet du kan tänka\n",
    "dig. Bland annat många möjligheter för att bearbeta text, statistik och grafisk\n",
    "presentation av resultaten. Python får mest av sin funktionalitet genom att\n",
    "importera 'bibliotek', och python har ett mycket starkt ekosystem med bibliotek\n",
    "som erbjuder nästan alla funktioner du kan tänka dig. Bland annat många\n",
    "möjligheter för att bearbeta text, statistik och grafisk presentation av\n",
    "resultaten.\n",
    "\n",
    "I denna workshop är de relevanta paketen:\n",
    "- Pandas: Ett kraftfullt bibliotek för datahantering.\n",
    "- Wordcloud: Ett Python-bibliotek för att generera ordmoln från text.\n",
    "- Matplotlib: Ett bibliotek för att skapa statiska, animerade och interaktiva\n",
    "  visualiseringar i Python.\n",
    "\n",
    "Vi kommer att installera dessa paket med hjälp av pip, Pythons pakethanterare.\n",
    "Pip är ett kommandoradsverktyg som gör det lätt att installera och hantera\n",
    "Python-paket.\n",
    "\n",
    "Vi använder också två bibliotek från _standardbiblioteket_:\n",
    "- re: Ett reguljärt uttrycksmodul -- för textmönstermatchning.\n",
    "- Counter: Ett objekt för att räkna förekomster av objekt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3vBjTYxQvr75",
    "outputId": "4fa23bab-5852-4908-d17e-3c7d7d17bc4f"
   },
   "outputs": [],
   "source": [
    "print(\"install and load libraries\")\n",
    "!python -m pip install pandas wordcloud matplotlib\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2BGwm7VGvr77"
   },
   "source": [
    "## Data – utterances about ABM\n",
    "\n",
    "The first thing we need is some text. \n",
    "We will here use _utterances_ from Swedish Parliamentary debates from 1867-2022.\n",
    "In order to find and access these _utterances_ we will use the[Riksdagsdebatter.se](https://riksdagsdebatter.se/public/index.html#/about) which offers a graphical interface for searching and accessing the debates.\n",
    "In order to find all the utterances mentioning ABM we search for the following three keywords:\n",
    "- `arkiv*`\n",
    "- `bibliotek*`\n",
    "- `muse*`\n",
    "Where `*` is a trunctation mark -- expanding to find *all words* starting with `arkiv`, `bibliotek`, or `muse` (1477 words according to the GUI).\n",
    "\n",
    "`Riksdagsdebatter.se` does not prove an API and the ~9k results are paginated at 50 results per page.\n",
    "However, the source material is available at: [the-swedish-parliament-corpus](https://github.com/swerik-project/the-swedish-parliament-corpus)\n",
    "where one can download the entire corpus and all speaker-metadata.\n",
    "In preparation for this workshop I have downloaded the records, filtered out all the utterances that do NOT mention at least one of our keywords and restructured the result into a CSV file. [scrip](https://gist.github.com/mathjoha/edcdaf57c5c2d58d9f6b58a6350b811d) [corpus](https://raw.githubusercontent.com/mathjoha/strik-og-kod/refs/heads/main/the-swedish-parliament-corpus_ABM_.csv)\n",
    "\n",
    "\n",
    "### Load data\n",
    "\n",
    "In order to access the data for the workshow it needs to be downloaded, which can easily be done with the `wget` program as such:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1HWgvDCvr79",
    "outputId": "8788e654-59ed-4e2d-e6ad-cc388cad4121"
   },
   "outputs": [],
   "source": [
    "!wget -c -nc \"https://github.com/mathjoha/strik-og-kod/blob/08f31ff60c9460efb232fecc1fbd07aace62dab4/data/the-swedish-parliament/the-swedish-parliament-corpus_ABM_.csv\" -O corpus.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yPpYXyFMvr8A"
   },
   "source": [
    "Then we use the function `read_csv` from the pandas library to load the file's contents into a DataFrame and keep that in memory under the variable name \"strik\".\n",
    "A DataFrame is comparable to spreadsheet in that it is a large matrix that stores data we work with.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MYSueAQ2vr8B"
   },
   "outputs": [],
   "source": [
    "riksdag = pd.read_csv(\"corpus.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMZ2G_Esvr8D"
   },
   "source": [
    "This gives us a new Pandas DataFrame named ”riksdag\" and containing 13755 rows and 5 columns.\n",
    "\n",
    "What is especially interesting for us is the column “content” – This is\n",
    "where the transcribed utterenaces are stored. Some of this text will not be easy\n",
    "on the eyes as they are filled with errors, and it is here where you meet the\n",
    "first downside of working with digitised text: OCR-errors.\n",
    "\n",
    "To understand why these errors occur it is necessary to turn towards the\n",
    "digitalization. In this process the protocols are scanned and processed \n",
    "with an OCR engine. These engines tend to work in two steps:\n",
    "\n",
    "  1. Segmenting the image into different blocks of text.\n",
    "  2. Transcribing the blocks of text into. \n",
    "\n",
    "These engines are typicall developed and tested on newer material -- part becase the older material is more complex, and there is less of it available. \n",
    "The team behin `The Swedish Parliament Corpus` have dedicated a lot of effort into developing systems for transcribing the older debates, identifying speakers and mappint them to metadata. Still, many [OCR-errors remain](https://github.com/swerik-project/pyriksdagen/blob/5cdc0875b7ed9a46f6ec7039d439d6e22e6acf54/examples/corpus-walkthrough.ipynb) in the material\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## The Text mining task\n",
    "\n",
    "\n",
    "First we will convert the text into lowercase and split it into words using [Regular expression](https://en.wikipedia.org/wiki/Regex).\n",
    "We store these `lists` of lowercase words in the DataFrame in the column `word` and we expand this column into a new dataframe where each word has its own row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofFT3Lzovr8F"
   },
   "outputs": [],
   "source": [
    "riksdag['word'] = riksdag.content.apply(lambda x: re.findall(r'\\w+', x.lower()))\n",
    "riksdag_tidy = riksdag.explode('word')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P1_QcrU1vr8G"
   },
   "source": [
    "Let us just print out the new data frame to see how the tidytext format looks in practice. This is achieved by writing the name of the data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iORiX19zvr8I",
    "outputId": "fb300f1b-83c3-4482-dd9f-50db5c3b3b79"
   },
   "outputs": [],
   "source": [
    "riksdag_tidy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZbMvJlVPvr8J"
   },
   "source": [
    "If we flip through the columns (with the little black arrow in the top-right corner) the last column will now be “word” which only contains single words.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "### Wordcloud\n",
    "\n",
    "To get an overview of our dataset we will begin by counting the most used words in the article about knitting in the period 1845 to 1850:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qy62aN8avr8K",
    "outputId": "02ae2e3c-c1ef-46a1-9f52-356d1cd1c0ef",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Counter(riksdag_tidy.word.values).most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIASMASdvr8L"
   },
   "source": [
    "<br> To no one’s surprise most frequent words in the dataset is the grammatical particles. One way to negate these words is by using a stopword list which can be used to remove unwanted words. For this we will use a stopwords-list published publicly by [@peterdalle](https://gist.github.com/peterdalle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XxQgt-lTvr8M",
    "outputId": "1a795c39-cbda-494f-e5c6-5ea7b61bf980"
   },
   "outputs": [],
   "source": [
    "!wget \"https://gist.githubusercontent.com/peterdalle/8865eb918a824a475b7ac5561f2f88e9/raw/cc1d05616e489576c1b934289711f041ff9b2281/swedish-stopwords.txt\" -O stopord.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KSiSK5ePvr8N"
   },
   "outputs": [],
   "source": [
    "with open('stopord.txt', 'r', encoding='utf-8') as f:\n",
    "  stopord = f.read().split('\\n')\n",
    "stopord += ['icke', 'af', 'herr', 'talman', 'år', 'ju', 't', '000']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aGkWL8mvr8N"
   },
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "We will filter out all the stopwords !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i00WiV0Rvr8O",
    "outputId": "25cb0732-3149-4bab-e590-587b211b8f35",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def not_stopword(word):\n",
    "  return word not in stopord\n",
    "\n",
    "words = Counter(filter(not_stopword, riksdag_tidy.word.values)).most_common()\n",
    "words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CpmR4_6avr8P"
   },
   "source": [
    "<br> We can already see quite a few interesting words. Something points\n",
    "towards a connection between maids that are seeking “condition” which back\n",
    "in the day meant a “service position” or a space of sorts. We can also see\n",
    "an OCR-error “eondition” and another spelling of condition, “kondition”.\n",
    "\n",
    "But a list is a little boring to look at. Could we perhaps create a\n",
    "beautiful wordcloud? Of course we can!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-n-LEhX9vr8P",
    "outputId": "57348a28-66b6-4de2-fd59-f97cc052d74b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wc = WordCloud()\n",
    "wc.generate_from_frequencies(\n",
    "  Counter(filter(not_stopword, riksdag_tidy.word.values))).to_file('swe_wc.png')\n",
    "plt.imshow(wc, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTLUwFB3vr8R"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
